{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Disease Prediction - Model Comparison\n",
    "\n",
    "This notebook demonstrates multiple machine learning models for heart disease prediction and compares their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../data/heart.csv')\n",
    "\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "print(\"Dataset Info:\")\n",
    "df.info()\n",
    "print(\"\\nStatistical Summary:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution\n",
    "print(\"Target Distribution:\")\n",
    "print(df['target'].value_counts())\n",
    "print(\"\\nTarget Proportions:\")\n",
    "print(df['target'].value_counts(normalize=True))\n",
    "\n",
    "# Visualize target distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x='target', data=df)\n",
    "plt.title('Distribution of Heart Disease')\n",
    "plt.xlabel('Target (0: No Disease, 1: Disease)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(df.corr(), annot=True, fmt='.2f', cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age distribution by target\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='target', y='age', data=df)\n",
    "plt.title('Age Distribution by Heart Disease Status')\n",
    "plt.xlabel('Target (0: No Disease, 1: Disease)')\n",
    "plt.ylabel('Age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chest pain type distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "pd.crosstab(df['cp'], df['target']).plot(kind='bar')\n",
    "plt.title('Chest Pain Type vs Heart Disease')\n",
    "plt.xlabel('Chest Pain Type')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(['No Disease', 'Disease'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Test set size:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, '../models/scaler.pkl')\n",
    "print(\"Scaler saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training and Evaluation\n",
    "\n",
    "We will train and evaluate the following models:\n",
    "1. Logistic Regression\n",
    "2. Random Forest\n",
    "3. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate models\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"Train and evaluate a model, returning metrics\"\"\"\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{model_name} Results\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"Cross-Validation Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "    \n",
    "    if y_pred_proba is not None:\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'{model_name} - Confusion Matrix')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()\n",
    "    \n",
    "    # ROC Curve\n",
    "    if y_pred_proba is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.4f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'{model_name} - ROC Curve')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'roc_auc': roc_auc if y_pred_proba is not None else None\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_results = evaluate_model(lr_model, X_train_scaled, X_test_scaled, y_train, y_test, \"Logistic Regression\")\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(lr_results['model'], '../models/logistic_regression_model.pkl')\n",
    "print(\"\\nLogistic Regression model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
    "rf_results = evaluate_model(rf_model, X_train, X_test, y_train, y_test, \"Random Forest\")\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(rf_results['model'], '../models/random_forest_model.pkl')\n",
    "print(\"\\nRandom Forest model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from Random Forest\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf_results['model'].feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance)\n",
    "plt.title('Feature Importance (Random Forest)')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 Important Features:\")\n",
    "print(feature_importance.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42, eval_metric='logloss')\n",
    "xgb_results = evaluate_model(xgb_model, X_train, X_test, y_train, y_test, \"XGBoost\")\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(xgb_results['model'], '../models/xgboost_model.pkl')\n",
    "print(\"\\nXGBoost model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from XGBoost\n",
    "feature_importance_xgb = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': xgb_results['model'].feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance_xgb)\n",
    "plt.title('Feature Importance (XGBoost)')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 Important Features (XGBoost):\")\n",
    "print(feature_importance_xgb.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. HRLFM (Hybrid Random Linear Forest Model)\n",
    "\n",
    "HRLFM combines Random Forest with Logistic Regression for hybrid predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HRLFM (Hybrid Random Linear Forest Model)\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "hrlfm_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)),\n",
    "        ('lr', LogisticRegression(random_state=42, max_iter=1000))\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# For HRLFM, we need to use scaled features for LR component\n",
    "hrlfm_results = evaluate_model(hrlfm_model, X_train, X_test, y_train, y_test, \"HRLFM\")\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(hrlfm_results['model'], '../models/hrlfm_model.pkl')\n",
    "print(\"\\nHRLFM model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "models_comparison = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Random Forest', 'XGBoost', 'HRLFM'],\n",
    "    'Accuracy': [lr_results['accuracy'], rf_results['accuracy'], xgb_results['accuracy'], hrlfm_results['accuracy']],\n",
    "    'Precision': [lr_results['precision'], rf_results['precision'], xgb_results['precision'], hrlfm_results['precision']],\n",
    "    'Recall': [lr_results['recall'], rf_results['recall'], xgb_results['recall'], hrlfm_results['recall']],\n",
    "    'F1-Score': [lr_results['f1'], rf_results['f1'], xgb_results['f1'], hrlfm_results['f1']],\n",
    "    'ROC-AUC': [lr_results['roc_auc'], rf_results['roc_auc'], xgb_results['roc_auc'], hrlfm_results['roc_auc']],\n",
    "    'CV Mean': [lr_results['cv_mean'], rf_results['cv_mean'], xgb_results['cv_mean'], hrlfm_results['cv_mean']]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(models_comparison.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "models_comparison_melted = models_comparison.melt(id_vars='Model', value_vars=metrics, \n",
    "                                                   var_name='Metric', value_name='Score')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Metric', y='Score', hue='Model', data=models_comparison_melted)\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Metric')\n",
    "plt.legend(title='Model')\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Best Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model based on F1-score\n",
    "best_model_idx = models_comparison['F1-Score'].idxmax()\n",
    "best_model_name = models_comparison.loc[best_model_idx, 'Model']\n",
    "best_f1_score = models_comparison.loc[best_model_idx, 'F1-Score']\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"BEST MODEL: {best_model_name}\")\n",
    "print(f\"F1-Score: {best_f1_score:.4f}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Save the best model info\n",
    "if best_model_name == 'Logistic Regression':\n",
    "    best_model = lr_results['model']\n",
    "elif best_model_name == 'Random Forest':\n",
    "    best_model = rf_results['model']\n",
    "elif best_model_name == 'XGBoost':\n",
    "    best_model = xgb_results['model']\n",
    "else:  # HRLFM\n",
    "    best_model = hrlfm_results['model']\n",
    "\n",
    "joblib.dump(best_model, '../models/best_model.pkl')\n",
    "print(f\"\\nBest model ({best_model_name}) saved as 'best_model.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sample Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a sample prediction\n",
    "sample_patient = X_test.iloc[0:1]\n",
    "print(\"Sample Patient Data:\")\n",
    "print(sample_patient)\n",
    "\n",
    "# For Logistic Regression (needs scaling)\n",
    "if best_model_name == 'Logistic Regression':\n",
    "    sample_scaled = scaler.transform(sample_patient)\n",
    "    prediction = best_model.predict(sample_scaled)\n",
    "    probability = best_model.predict_proba(sample_scaled)[0]\n",
    "else:\n",
    "    prediction = best_model.predict(sample_patient)\n",
    "    probability = best_model.predict_proba(sample_patient)[0]\n",
    "\n",
    "print(f\"\\nPrediction: {'Heart Disease' if prediction[0] == 1 else 'No Heart Disease'}\")\n",
    "print(f\"Probability - No Disease: {probability[0]:.4f}, Disease: {probability[1]:.4f}\")\n",
    "print(f\"\\nActual: {'Heart Disease' if y_test.iloc[0] == 1 else 'No Heart Disease'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "In this notebook, we:\n",
    "1. Loaded and explored the heart disease dataset\n",
    "2. Performed exploratory data analysis\n",
    "3. Trained and evaluated multiple machine learning models:\n",
    "   - Logistic Regression\n",
    "   - Random Forest\n",
    "   - XGBoost\n",
    "4. Compared model performances\n",
    "5. Selected the best model based on F1-score\n",
    "6. Saved all models for deployment\n",
    "\n",
    "The models are now ready to be used in the Streamlit application for real-time predictions!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}